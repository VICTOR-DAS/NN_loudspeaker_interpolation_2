{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b28c948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, patch_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.projection = layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size)\n",
    "        self.flatten = layers.Reshape((-1, embed_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        patches = self.projection(x)\n",
    "        flattened = self.flatten(patches)\n",
    "        return flattened\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Multiply\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def attention_block(x, g, inter_channel):\n",
    "    \"\"\"\n",
    "    x: Skip connection input\n",
    "    g: Gate signal (from previous layer)\n",
    "    inter_channel: Number of intermediate channels\n",
    "    \"\"\"\n",
    "    # Get input shapes\n",
    "    g_shape = K.int_shape(g)\n",
    "    x_shape = K.int_shape(x)\n",
    "    \n",
    "    # Calculate target dimensions\n",
    "    target_h = x_shape[1]\n",
    "    target_w = x_shape[2]\n",
    "    \n",
    "    # Ensure inter_channel is valid\n",
    "    inter_channel = max(1, int(inter_channel))\n",
    "    \n",
    "    # Resize gate signal if needed\n",
    "    if g_shape[1] != target_h or g_shape[2] != target_w:\n",
    "        g = tf.image.resize(g, (target_h, target_w))\n",
    "        if g_shape[3] != x_shape[3]:\n",
    "            g = Conv2D(x_shape[3], (1, 1), padding='same')(g)\n",
    "    \n",
    "    # Transform signals\n",
    "    theta_x = Conv2D(inter_channel, (1, 1), padding='same')(x)\n",
    "    phi_g = Conv2D(inter_channel, (1, 1), padding='same')(g)\n",
    "    \n",
    "    # Compute compatibility\n",
    "    f = LeakyReLU(alpha=0.3)(theta_x + phi_g)\n",
    "    psi_f = Conv2D(1, (1, 1), padding='same')(f)\n",
    "    \n",
    "    # Generate attention weights\n",
    "    rate = tf.nn.sigmoid(psi_f)\n",
    "    \n",
    "    # Apply attention\n",
    "    att_x = Multiply()([x, rate])\n",
    "    \n",
    "    return att_x\n",
    "\n",
    "def unet_transformer_inpainting(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Initial patch embedding\n",
    "    patch_size = (72, 4)\n",
    "    embed_dim = 256\n",
    "    x = PatchEmbedding(patch_size, embed_dim)(inputs)\n",
    "    \n",
    "    # Transformer blocks\n",
    "    transformer_layers = []\n",
    "    for _ in range(4):\n",
    "        x = TransformerBlock(embed_dim, num_heads=8, ff_dim=4*embed_dim)(x)\n",
    "        transformer_layers.append(x)\n",
    "    \n",
    "    # Reshape back to spatial dimensions\n",
    "    x = layers.Reshape((IMG_HEIGHT//patch_size[0], IMG_WIDTH//patch_size[1], embed_dim))(x)\n",
    "    \n",
    "\n",
    "    # importar inicializador de semilla\n",
    "    from tensorflow.keras.initializers import RandomNormal, RandomUniform, TruncatedNormal, VarianceScaling, glorot_normal, glorot_uniform, he_normal, he_uniform\n",
    "\n",
    "    # Importar funciones de activación\n",
    "    from tensorflow.keras.layers import LeakyReLU, PReLU, ReLU, ELU, ThresholdedReLU\n",
    "\n",
    "    # Definir inicializador con semilla\n",
    "    initializer = glorot_normal(seed=0)\n",
    "\n",
    "    # Definir función de activación\n",
    "    activation = LeakyReLU(alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    kernel_size = (3, 3)\n",
    "    # Capa de entrada\n",
    "\n",
    "\n",
    "    # Bloque 1\n",
    "    conv1 = Conv2D(32, (1500,6), activation=activation, padding='same', kernel_initializer=initializer)(inputs)\n",
    "    conv1 = Conv2D(32, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    # Bloque 2\n",
    "    conv2 = Conv2D(64, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(pool1)\n",
    "    conv2 = Conv2D(64, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 1))(conv2)\n",
    "\n",
    "    # Bloque 3\n",
    "    conv3 = Conv2D(128, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(pool2)\n",
    "    conv3 = Conv2D(128, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 1))(conv3)\n",
    "\n",
    "    # Bloque 4\n",
    "    conv4 = Conv2D(256, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(pool3)\n",
    "    conv4 = Conv2D(256, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv4)\n",
    "    drop4 = Dropout(0)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(3, 2))(drop4)\n",
    "\n",
    "    # Bloque 5\n",
    "    conv41 = Conv2D(512, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(pool4)\n",
    "    conv41 = Conv2D(512, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv41)\n",
    "    drop41 = Dropout(0.01)(conv41)\n",
    "    pool41 = MaxPooling2D(pool_size=(3, 1))(drop41)\n",
    "\n",
    "\n",
    "\n",
    "    # Capa de bottleneck\n",
    "    conv5 = Conv2D(1024, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(pool41)\n",
    "    conv5 = Conv2D(1024, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv5)\n",
    "\n",
    "    # Bloque 61\n",
    "    up61 = Conv2DTranspose(512, kernel_size, strides=(3, 1), padding='same', kernel_initializer=initializer)(conv5)\n",
    "    up61 = concatenate([up61, drop41], axis=3)\n",
    "    conv61 = Conv2D(512, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(up61)\n",
    "    conv61 = Conv2D(512, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv61)\n",
    "\n",
    "    # Bloque 6\n",
    "    up6 = Conv2DTranspose(256, kernel_size, strides=(3, 2), padding='same', kernel_initializer=initializer)(conv61)\n",
    "    up6 = concatenate([up6, drop4], axis=3)\n",
    "    conv6 = Conv2D(256, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(up6)\n",
    "    conv6 = Conv2D(256, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv6)\n",
    "\n",
    "    # Bloque 7\n",
    "    up7 = Conv2DTranspose(128, kernel_size, strides=(2, 1), padding='same', kernel_initializer=initializer)(conv6)\n",
    "    up7 = concatenate([up7, conv3], axis=3)\n",
    "    conv7 = Conv2D(128, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(up7)\n",
    "    conv7 = Conv2D(128, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv7)\n",
    "\n",
    "    # Bloque 8\n",
    "    up8 = Conv2DTranspose(64, kernel_size, strides=(2, 1), padding='same', kernel_initializer=initializer)(conv7)\n",
    "    up8 = concatenate([up8, conv2], axis=3)\n",
    "    conv8 = Conv2D(64, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(up8)\n",
    "    conv8 = Conv2D(64, kernel_size, activation=activation, padding='same', kernel_initializer=initializer)(conv8)\n",
    "\n",
    "    # Bloque 9\n",
    "    up9 = Conv2DTranspose(32, kernel_size, strides=(2, 2), padding='same', kernel_initializer=initializer)(conv8)\n",
    "    up9 = concatenate([up9, conv1], axis=3)\n",
    "    conv9 = Conv2D(16, (1,1), activation=activation, padding='same', kernel_initializer=initializer)(up9)\n",
    "    conv10 = Conv2D(16, (1,1), activation=activation, padding='same', kernel_initializer=initializer)(conv9)\n",
    "    conv11 = Conv2D(1, (1,1), padding='same', activation=activation, kernel_initializer=initializer)(conv10)\n",
    "    \n",
    "\n",
    "        # Add global attention at skip connections\n",
    "    for i, skip in enumerate([conv4, conv3, conv2, conv1]):\n",
    "        x = attention_block(skip, x, embed_dim//(2**i))\n",
    "        x = concatenate([x, skip], axis=3)\n",
    "    \n",
    "    outputs = Conv2D(1, (1,1), activation='relu')(x)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Definir el modelo\n",
    "\n",
    "model = unet_transformer_inpainting(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# obtener el índice de ejecución actual\n",
    "cell_index=(get_ipython().execution_count)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
